#!/usr/bin/env python
from __future__ import print_function

import argparse
import os
import sys
import json
import pprint
import time

from tasks import *

from doit.cmd_base import TaskLoader
from doit.doit_cmd import DoitMain

def run_tasks(tasks, args, config={'verbosity': 2}):
    
    if type(tasks) is not list:
        raise TypeError('tasks must be a list')
   
    class Loader(TaskLoader):
        @staticmethod
        def load_tasks(cmd, opt_values, pos_args):
            return tasks, config
   
    DoitMain(Loader()).run(args)

CUR_TIME = time.strftime('%Y-%m-%d-%H%M')

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resources', default='resources.json')
    parser.add_argument('--config', default='config.json')
    parser.add_argument('--print-tasks', action='store_true', default=False)
    parser.add_argument('--data-dir', default='_data')
    parser.add_argument('--work-dir', default='_work')
    args, doit_args = parser.parse_known_args()

    with open(args.resources, 'r') as fp:
        print('** Using data resources found in {c}'.format(c=args.resources), file=sys.stderr)
        resources = json.load(fp)
    with open(args.config, 'r') as fp:
        print('** Using config found in {c}'.format(c=args.config), file=sys.stderr)
        config = json.load(fp)

    desc = '''
####################################################################

 2015 Fish DNA assembly Pipeline

 * Authors:
   {authors}

 * About:
   {desc}

####################################################################
'''.format(authors=', '.join(config['meta']['authors']),
           desc=config['meta']['description'])
    print(desc, file=sys.stderr)

    data_dir = os.path.abspath(args.data_dir)
    work_dir = os.path.abspath(args.work_dir)

    resources_df = pd.DataFrame(resources).transpose()

    old_dir = os.getcwd()
    try:
        if not os.path.exists(work_dir):
            os.makedirs(work_dir)
        os.chdir(work_dir)
        print('** Current Working Directory: {w}'.format(w=os.getcwd()), file=sys.stderr)
        print('** Current Data Directory: {d}\n'.format(d=data_dir), file=sys.stderr)

        tasks = []

        # Download the samples. Might take a loooooooong time!
        tasks.append(create_folder_task(data_dir))
        sample_df = resources_df[resources_df.meta_type == 'sample']
        for key, row in sample_df.iterrows():
            tasks.append(download_task(row.url, os.path.join(data_dir, row.filename)))

        # Link samples into the working dir
        for key, row in sample_df.iterrows():
            tasks.append(link_file_task(os.path.join(data_dir, row.filename)))
        
        # Run first assessment of sample quality
        fastqc_tasks = []
        fastqc_cfg = config['pipeline']['fastqc']
        for key, row in sample_df.iterrows():
            fastqc_tasks.append(fastqc_task(row.filename, fastqc_cfg))
        tasks.extend(fastqc_tasks)
        tasks.append(group_task('fastqc-raw', [t.name for t in fastqc_tasks]))

        # Run trimmomatic on the samples
        qc_files = []
        trim_cfg = config['pipeline']['trimmomatic']
        adapter_row = resources_df.ix['adapters']
        tasks.append(download_and_gunzip_task(adapter_row.url, adapter_row.filename))
        for sample, sample_group in resources_df.groupby('sample'):
            s = sample_group.pivot('sample', 'fragment', 'filename')
            left = s.left[0]
            right = s.right[0]
            tasks.append(trimmomatic_pe_task(left, right, left + '.paired', left + '.unpaired', 
                                             right + '.paired', right + '.unpaired', adapter_row.filename,
                                             'phred33', trim_cfg))
            tasks.append(interleave_task(left + '.paired', right + '.paired', sample + '.qc'))
            qc_files.append(sample + '.qc')

        # Load the samples into a countgraph
        counting_cfg = config['pipeline']['khmer']['counting']
        qc_ct_fn = config['pipeline']['prefix'] + '.qc.ct'
        tasks.append(load_counting_task(qc_files, qc_ct_fn, counting_cfg, 'qc-fish'))


        # Run two-pass diginorm on the samples using the countgraph
        dg_cfg = config['pipeline']['khmer']['normalization']
        final_files = []
        for fn in qc_files:
            output_fn = fn + '.C{}'.format(dg_cfg['coverage'])
            tasks.append(diginorm_task(fn, output_fn, dg_cfg, load_ct=qc_ct_fn))
            tasks.append(link_file_task(output_fn, output_fn+'.fq'))
            final_files.append(output_fn + '.fq')

        # Generate abundance histograms
        for fn in final_files:
            tasks.append(abundance_dist_task(fn, qc_ct_fn, fn + '.hist'))

        # Run KmerGenie on the processed samples
        kmgenie_cfg = config['pipeline']['kmergenie']
        kmgenie_task = kmergenie_task(final_files, kmgenie_cfg)
        tasks.append(kmgenie_task)
        tasks.append(group_task('kmergenie', [kmgenie_task.name]))
        
        # Perform a round of FastQC on the processed samples
        fastqc_final_tasks = []
        for fn in final_files:
            fastqc_final_tasks.append(fastqc_task(fn, fastqc_cfg))
        tasks.extend(fastqc_final_tasks)
        tasks.append(group_task('fastqc-final', [t.name for t in fastqc_final_tasks]))

        # Generate the velvet submission script
        velvet_template = os.path.join(old_dir, config['pipeline']['velvet']['template_file'])
        tasks.append(build_velvet_task(final_files, velvet_template, CUR_TIME,
                                       config['pipeline']['velvet'],
                                       config['pipeline']['pbs-params'], 
                                       label='velvet_' + config['pipeline']['prefix']))

        # Generate the spades submission script
        spades_template = os.path.join(old_dir, config['pipeline']['spades']['template_file'])
        tasks.append(build_spades_task(final_files, spades_template, CUR_TIME,
                                       config['pipeline']['spades'],
                                       config['pipeline']['pbs-params'],
                                       label='spades_' + config['pipeline']['prefix']))
        
        #for infn, outfn in zip(dg_files, abyss_files):
        #    tasks.append(format_abyss_task(infn, outfn))

        #tasks.append(submit_abyss_task(abyss_files, config['pipeline']['abyss'],
        #             config['pipeline']['pbs-params'], label='abyss-' + config['pipeline']['prefix']))

        if args.print_tasks:
            for task in tasks:
                print('-----\n', task)
                pprint.pprint(task.__dict__)

        run_tasks(tasks, doit_args)

    finally:
        os.chdir(old_dir)

main()
